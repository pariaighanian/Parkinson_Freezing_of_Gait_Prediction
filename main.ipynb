{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_processing import (\n",
    "    load_sensor_data, \n",
    "    load_meta_data, \n",
    "    read_sensor_data, \n",
    "    read_meta_data, \n",
    "    add_diagnosis_to_sensor, \n",
    "    filter_and_label_diagnosis,\n",
    "    add_feature_to_sensor,\n",
    "    convert_yob_to_age,\n",
    "    impute_age_by_patient_avg,\n",
    "    binary_gender_encode,\n",
    "    impute_updrs3_by_knn,\n",
    "    split_dataset,\n",
    "    normalize_data\n",
    ")\n",
    "from data_visuals import plot_sensor_signals\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \"G:/My Drive/fog_dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: G:/My Drive/fog_dataset/diagnosis\n",
      "\n",
      "Processing task: TUG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Sensor Data for TUG: 100%|██████████| 122/122 [00:31<00:00,  3.86it/s]\n",
      "Loading Metadata for TUG: 100%|██████████| 122/122 [00:05<00:00, 24.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing task: 2minwalk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Sensor Data for 2minwalk: 100%|██████████| 122/122 [01:31<00:00,  1.34it/s]\n",
      "Loading Metadata for 2minwalk: 100%|██████████| 122/122 [00:05<00:00, 23.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing task: 4x10mFastWithStop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Sensor Data for 4x10mFastWithStop: 100%|██████████| 122/122 [00:39<00:00,  3.09it/s]\n",
      "Loading Metadata for 4x10mFastWithStop: 100%|██████████| 122/122 [00:05<00:00, 22.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing task: 4x10mPrefWithoutStop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Sensor Data for 4x10mPrefWithoutStop: 100%|██████████| 122/122 [00:45<00:00,  2.67it/s]\n",
      "Loading Metadata for 4x10mPrefWithoutStop: 100%|██████████| 122/122 [00:05<00:00, 24.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing task: 4x10mSlowWithStop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Sensor Data for 4x10mSlowWithStop: 100%|██████████| 122/122 [00:55<00:00,  2.18it/s]\n",
      "Loading Metadata for 4x10mSlowWithStop: 100%|██████████| 122/122 [00:05<00:00, 24.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All tasks processed and saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "diagnosis_dir = os.path.join(root_directory, \"diagnosis\")\n",
    "\n",
    "tasks = [\"TUG\", \"2minwalk\", \"4x10mFastWithStop\", \"4x10mPrefWithoutStop\", \"4x10mSlowWithStop\"]\n",
    "\n",
    "if not os.path.exists(diagnosis_dir):\n",
    "    os.makedirs(diagnosis_dir)\n",
    "    print(f\"Created directory: {diagnosis_dir}\")\n",
    "\n",
    "for task_name in tasks:\n",
    "    print(f\"\\nProcessing task: {task_name}...\")\n",
    "\n",
    "    sensor_save_path = os.path.join(diagnosis_dir, f\"sensor_{task_name}.csv\")\n",
    "    meta_save_path = os.path.join(diagnosis_dir, f\"meta_{task_name}.csv\")\n",
    "\n",
    "    if os.path.exists(sensor_save_path) and os.path.exists(meta_save_path):\n",
    "        print(f\"Files for {task_name} already exist. Skipping...\")\n",
    "        continue  \n",
    "\n",
    "    if not os.path.exists(sensor_save_path):\n",
    "        try:\n",
    "            sensor_df = load_sensor_data(root_directory, task_name)\n",
    "            if not sensor_df.empty:\n",
    "                sensor_df.to_csv(sensor_save_path, index=False)\n",
    "            else:\n",
    "                print(f\"No sensor data found for {task_name}. Skipping...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sensor data for {task_name}: {e}\")\n",
    "\n",
    "    if not os.path.exists(meta_save_path):\n",
    "        try:\n",
    "            meta_df = load_meta_data(root_directory, task_name)\n",
    "            if not meta_df.empty:\n",
    "                meta_df.to_csv(meta_save_path, index=False)\n",
    "            else:\n",
    "                print(f\"No metadata found for {task_name}. Skipping...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading metadata for {task_name}: {e}\")\n",
    "\n",
    "print(\"\\nAll tasks processed and saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading sensor data for TUG...\n",
      "Reading metadata for TUG...\n",
      "Reading sensor data for 2minwalk...\n",
      "Reading metadata for 2minwalk...\n",
      "Reading sensor data for 4x10mSlowWithStop...\n",
      "Reading metadata for 4x10mSlowWithStop...\n",
      "Reading sensor data for 4x10mPrefWithoutStop...\n",
      "Reading metadata for 4x10mPrefWithoutStop...\n",
      "Reading sensor data for 4x10mFastWithStop...\n",
      "Reading metadata for 4x10mFastWithStop...\n"
     ]
    }
   ],
   "source": [
    "sensor_tug = read_sensor_data(root_directory, \"TUG\")\n",
    "meta_tug = read_meta_data(root_directory, \"TUG\")\n",
    "\n",
    "sensor_2minwalk = read_sensor_data(root_directory, \"2minwalk\")\n",
    "meta_2minwalk = read_meta_data(root_directory, \"2minwalk\")\n",
    "\n",
    "sensor_slow = read_sensor_data(root_directory, \"4x10mSlowWithStop\")\n",
    "meta_slow = read_meta_data(root_directory, \"4x10mSlowWithStop\")\n",
    "\n",
    "sensor_pref = read_sensor_data(root_directory, \"4x10mPrefWithoutStop\")\n",
    "meta_pref = read_meta_data(root_directory, \"4x10mPrefWithoutStop\")\n",
    "\n",
    "sensor_fast = read_sensor_data(root_directory, \"4x10mFastWithStop\")\n",
    "meta_fast = read_meta_data(root_directory, \"4x10mFastWithStop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnosis Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add diagnosis to sensor data\n",
    "sensor_tug = add_diagnosis_to_sensor(sensor_tug, meta_tug)\n",
    "sensor_2minwalk = add_diagnosis_to_sensor(sensor_2minwalk, meta_2minwalk)\n",
    "sensor_slow = add_diagnosis_to_sensor(sensor_slow, meta_slow)\n",
    "sensor_pref = add_diagnosis_to_sensor(sensor_pref, meta_pref)\n",
    "sensor_fast = add_diagnosis_to_sensor(sensor_fast, meta_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_to_keep = [\n",
    "    \"Parkinson's disease\",\n",
    "    \"Control\",\n",
    "    \"Parkinson's disease and dementia\",\n",
    "    \"Parkinsonism unspecified\",\n",
    "    \"Secondary parkinsonism: other\"\n",
    "]\n",
    "\n",
    "label_1_diagnoses = [\n",
    "    \"Parkinson's disease\",\n",
    "    \"Parkinson's disease and dementia\",\n",
    "    \"Parkinsonism unspecified\",\n",
    "    \"Secondary parkinsonism: other\"\n",
    "]\n",
    "\n",
    "label_0_diagnoses = [\"Control\"]\n",
    "\n",
    "# Apply filtering & mapping (Diagnosis column will now be 0 or 1)\n",
    "sensor_tug = filter_and_label_diagnosis(sensor_tug, diagnosis_to_keep, label_1_diagnoses, label_0_diagnoses)\n",
    "sensor_2minwalk = filter_and_label_diagnosis(sensor_2minwalk, diagnosis_to_keep, label_1_diagnoses, label_0_diagnoses)\n",
    "sensor_slow = filter_and_label_diagnosis(sensor_slow, diagnosis_to_keep, label_1_diagnoses, label_0_diagnoses)\n",
    "sensor_pref = filter_and_label_diagnosis(sensor_pref, diagnosis_to_keep, label_1_diagnoses, label_0_diagnoses)\n",
    "sensor_fast = filter_and_label_diagnosis(sensor_fast, diagnosis_to_keep, label_1_diagnoses, label_0_diagnoses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Year of Birth (YOB) to sensor data \n",
    "sensor_tug = add_feature_to_sensor(sensor_tug, meta_tug, \"YOB\")\n",
    "sensor_2minwalk = add_feature_to_sensor(sensor_2minwalk, meta_2minwalk, \"YOB\")\n",
    "sensor_slow = add_feature_to_sensor(sensor_slow, meta_slow, \"YOB\")\n",
    "sensor_pref = add_feature_to_sensor(sensor_pref, meta_pref, \"YOB\")\n",
    "sensor_fast = add_feature_to_sensor(sensor_fast, meta_fast, \"YOB\")\n",
    "\n",
    "# Convert YOB to Age\n",
    "sensor_tug = convert_yob_to_age(sensor_tug)\n",
    "sensor_2minwalk = convert_yob_to_age(sensor_2minwalk)\n",
    "sensor_slow = convert_yob_to_age(sensor_slow)\n",
    "sensor_pref = convert_yob_to_age(sensor_pref)\n",
    "sensor_fast = convert_yob_to_age(sensor_fast)\n",
    "\n",
    "# Impute missing Age values based on unique patient averages \n",
    "sensor_tug = impute_age_by_patient_avg(sensor_tug)\n",
    "sensor_2minwalk = impute_age_by_patient_avg(sensor_2minwalk)\n",
    "sensor_slow = impute_age_by_patient_avg(sensor_slow)\n",
    "sensor_pref = impute_age_by_patient_avg(sensor_pref)\n",
    "sensor_fast = impute_age_by_patient_avg(sensor_fast)\n",
    "\n",
    "# Drop YOB\n",
    "sensor_tug.drop(columns=[\"YOB\"], inplace=True)\n",
    "sensor_2minwalk.drop(columns=[\"YOB\"], inplace=True)\n",
    "sensor_slow.drop(columns=[\"YOB\"], inplace=True)\n",
    "sensor_pref.drop(columns=[\"YOB\"], inplace=True)\n",
    "sensor_fast.drop(columns=[\"YOB\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Gender to sensor data\n",
    "sensor_tug = add_feature_to_sensor(sensor_tug, meta_tug, \"Gender\")\n",
    "sensor_2minwalk = add_feature_to_sensor(sensor_2minwalk, meta_2minwalk, \"Gender\")\n",
    "sensor_slow = add_feature_to_sensor(sensor_slow, meta_slow, \"Gender\")\n",
    "sensor_pref = add_feature_to_sensor(sensor_pref, meta_pref, \"Gender\")\n",
    "sensor_fast = add_feature_to_sensor(sensor_fast, meta_fast, \"Gender\")\n",
    "\n",
    "# Convert Gender to binary \n",
    "sensor_tug = binary_gender_encode(sensor_tug)\n",
    "sensor_2minwalk = binary_gender_encode(sensor_2minwalk)\n",
    "sensor_slow = binary_gender_encode(sensor_slow)\n",
    "sensor_pref = binary_gender_encode(sensor_pref)\n",
    "sensor_fast = binary_gender_encode(sensor_fast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add UPDRS3 to sensor data\n",
    "sensor_tug = add_feature_to_sensor(sensor_tug, meta_tug, \"MSDS-UPDRS part 3\").rename(columns={\"MSDS-UPDRS part 3\": \"UPDRS3\"})\n",
    "sensor_2minwalk = add_feature_to_sensor(sensor_2minwalk, meta_2minwalk, \"MSDS-UPDRS part 3\").rename(columns={\"MSDS-UPDRS part 3\": \"UPDRS3\"})\n",
    "sensor_slow = add_feature_to_sensor(sensor_slow, meta_slow, \"MSDS-UPDRS part 3\").rename(columns={\"MSDS-UPDRS part 3\": \"UPDRS3\"})\n",
    "sensor_pref = add_feature_to_sensor(sensor_pref, meta_pref, \"MSDS-UPDRS part 3\").rename(columns={\"MSDS-UPDRS part 3\": \"UPDRS3\"})\n",
    "sensor_fast = add_feature_to_sensor(sensor_fast, meta_fast, \"MSDS-UPDRS part 3\").rename(columns={\"MSDS-UPDRS part 3\": \"UPDRS3\"})\n",
    "\n",
    "# Impute missing UPDRS3 values \n",
    "sensor_tug = impute_updrs3_by_knn(sensor_tug)\n",
    "sensor_2minwalk = impute_updrs3_by_knn(sensor_2minwalk)\n",
    "sensor_slow = impute_updrs3_by_knn(sensor_slow)\n",
    "sensor_pref = impute_updrs3_by_knn(sensor_pref)\n",
    "sensor_fast = impute_updrs3_by_knn(sensor_fast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found existing split files for TUG. Loading data...\n",
      "Dataset Split Summary for TUG:\n",
      "  - Total Patients: 112 (Original)\n",
      "  - Train Patients: 71\n",
      "  - Validation Patients: 18\n",
      "  - Test Patients: 23\n",
      "\n",
      "Found existing split files for 2minwalk. Loading data...\n",
      "Dataset Split Summary for 2minwalk:\n",
      "  - Total Patients: 115 (Original)\n",
      "  - Train Patients: 73\n",
      "  - Validation Patients: 19\n",
      "  - Test Patients: 23\n",
      "\n",
      "Found existing split files for Slow. Loading data...\n",
      "Dataset Split Summary for Slow:\n",
      "  - Total Patients: 110 (Original)\n",
      "  - Train Patients: 70\n",
      "  - Validation Patients: 18\n",
      "  - Test Patients: 22\n",
      "\n",
      "Found existing split files for Pref. Loading data...\n",
      "Dataset Split Summary for Pref:\n",
      "  - Total Patients: 109 (Original)\n",
      "  - Train Patients: 69\n",
      "  - Validation Patients: 18\n",
      "  - Test Patients: 22\n",
      "\n",
      "Found existing split files for Fast. Loading data...\n",
      "Dataset Split Summary for Fast:\n",
      "  - Total Patients: 110 (Original)\n",
      "  - Train Patients: 70\n",
      "  - Validation Patients: 18\n",
      "  - Test Patients: 22\n"
     ]
    }
   ],
   "source": [
    "df_dict = {\n",
    "    \"TUG\": sensor_tug,\n",
    "    \"2minwalk\": sensor_2minwalk,\n",
    "    \"Slow\": sensor_slow,\n",
    "    \"Pref\": sensor_pref,\n",
    "    \"Fast\": sensor_fast,\n",
    "}\n",
    "\n",
    "save_paths = {task: f\"{root_directory}/diagnosis/df_{task}\" for task in df_dict.keys()}\n",
    "\n",
    "split_results = {}\n",
    "\n",
    "# Perform train-validation-test split \n",
    "for task_name, df in df_dict.items():\n",
    "    split_results[task_name] = split_dataset(df, dataset_name=task_name, save_path=save_paths[task_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_columns = [\n",
    "    'L_AccX_filt', 'L_AccY_filt', 'L_AccZ_filt',\n",
    "    'L_GyrX_filt', 'L_GyrY_filt', 'L_GyrZ_filt',\n",
    "    'R_AccX_filt', 'R_AccY_filt', 'R_AccZ_filt',\n",
    "    'R_GyrX_filt', 'R_GyrY_filt', 'R_GyrZ_filt'\n",
    "]\n",
    "\n",
    "# Apply normalization for Train, Validation, and Test sets\n",
    "for task_name, splits in split_results.items():\n",
    "    split_results[task_name][\"train\"] = normalize_data(splits[\"train\"], sensor_columns)\n",
    "    split_results[task_name][\"val\"] = normalize_data(splits[\"val\"], sensor_columns)\n",
    "    split_results[task_name][\"test\"] = normalize_data(splits[\"test\"], sensor_columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
