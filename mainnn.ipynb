{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_processing import (\n",
    "    load_sensor_data, \n",
    "    load_meta_data, \n",
    "    read_sensor_data, \n",
    "    read_meta_data, \n",
    "    add_diagnosis_to_sensor, \n",
    "    filter_and_label_diagnosis,\n",
    "    add_feature_to_sensor,\n",
    "    convert_yob_to_age,\n",
    "    impute_age_by_patient_avg,\n",
    "    binary_gender_encode,\n",
    "    impute_updrs3_by_knn,\n",
    "    split_dataset,\n",
    "    normalize_data,\n",
    "    create_fixed_windows_with_overlap\n",
    ")\n",
    "\n",
    "from feature_extraction import (\n",
    "    apply_fft_to_windows,\n",
    "    extract_time_domain_features,\n",
    "    extract_fft_features\n",
    ")\n",
    "from data_visuals import plot_sensor_signals\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \"G:/My Drive/fog_dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: G:/My Drive/fog_dataset/diagnosis\n",
      "\n",
      "Processing task: TUG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Sensor Data for TUG: 100%|██████████| 122/122 [00:20<00:00,  6.01it/s]\n",
      "Loading Metadata for TUG: 100%|██████████| 122/122 [00:02<00:00, 42.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing task: 2minwalk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Sensor Data for 2minwalk: 100%|██████████| 122/122 [00:41<00:00,  2.93it/s]\n",
      "Loading Metadata for 2minwalk: 100%|██████████| 122/122 [00:02<00:00, 44.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing task: 4x10mFastWithStop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Sensor Data for 4x10mFastWithStop: 100%|██████████| 122/122 [00:24<00:00,  4.94it/s]\n",
      "Loading Metadata for 4x10mFastWithStop: 100%|██████████| 122/122 [00:02<00:00, 43.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing task: 4x10mPrefWithoutStop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Sensor Data for 4x10mPrefWithoutStop: 100%|██████████| 122/122 [00:23<00:00,  5.19it/s]\n",
      "Loading Metadata for 4x10mPrefWithoutStop: 100%|██████████| 122/122 [00:02<00:00, 42.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing task: 4x10mSlowWithStop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Sensor Data for 4x10mSlowWithStop: 100%|██████████| 122/122 [00:29<00:00,  4.14it/s]\n",
      "Loading Metadata for 4x10mSlowWithStop: 100%|██████████| 122/122 [00:03<00:00, 40.00it/s]\n"
     ]
    }
   ],
   "source": [
    "diagnosis_dir = os.path.join(root_directory, \"diagnosis\")\n",
    "\n",
    "tasks = [\"TUG\", \"2minwalk\", \"4x10mFastWithStop\", \"4x10mPrefWithoutStop\", \"4x10mSlowWithStop\"]\n",
    "\n",
    "if not os.path.exists(diagnosis_dir):\n",
    "    os.makedirs(diagnosis_dir)\n",
    "    print(f\"Created directory: {diagnosis_dir}\")\n",
    "\n",
    "for task_name in tasks:\n",
    "    print(f\"\\nProcessing task: {task_name}...\")\n",
    "\n",
    "    sensor_save_path = os.path.join(diagnosis_dir, f\"sensor_{task_name}.csv\")\n",
    "    meta_save_path = os.path.join(diagnosis_dir, f\"meta_{task_name}.csv\")\n",
    "\n",
    "    if os.path.exists(sensor_save_path) and os.path.exists(meta_save_path):\n",
    "        print(f\"Files for {task_name} already exist. Skipping...\")\n",
    "        continue  \n",
    "\n",
    "    if not os.path.exists(sensor_save_path):\n",
    "        try:\n",
    "            sensor_df = load_sensor_data(root_directory, task_name)\n",
    "            if not sensor_df.empty:\n",
    "                sensor_df.to_csv(sensor_save_path, index=False)\n",
    "            else:\n",
    "                print(f\"No sensor data found for {task_name}. Skipping...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sensor data for {task_name}: {e}\")\n",
    "\n",
    "    if not os.path.exists(meta_save_path):\n",
    "        try:\n",
    "            meta_df = load_meta_data(root_directory, task_name)\n",
    "            if not meta_df.empty:\n",
    "                meta_df.to_csv(meta_save_path, index=False)\n",
    "            else:\n",
    "                print(f\"No metadata found for {task_name}. Skipping...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading metadata for {task_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading sensor data for TUG...\n",
      "Reading metadata for TUG...\n",
      "Reading sensor data for 2minwalk...\n",
      "Reading metadata for 2minwalk...\n",
      "Reading sensor data for 4x10mSlowWithStop...\n",
      "Reading metadata for 4x10mSlowWithStop...\n",
      "Reading sensor data for 4x10mPrefWithoutStop...\n",
      "Reading metadata for 4x10mPrefWithoutStop...\n",
      "Reading sensor data for 4x10mFastWithStop...\n",
      "Reading metadata for 4x10mFastWithStop...\n"
     ]
    }
   ],
   "source": [
    "sensor_tug = read_sensor_data(root_directory, \"TUG\")\n",
    "meta_tug = read_meta_data(root_directory, \"TUG\")\n",
    "\n",
    "sensor_2minwalk = read_sensor_data(root_directory, \"2minwalk\")\n",
    "meta_2minwalk = read_meta_data(root_directory, \"2minwalk\")\n",
    "\n",
    "sensor_slow = read_sensor_data(root_directory, \"4x10mSlowWithStop\")\n",
    "meta_slow = read_meta_data(root_directory, \"4x10mSlowWithStop\")\n",
    "\n",
    "sensor_pref = read_sensor_data(root_directory, \"4x10mPrefWithoutStop\")\n",
    "meta_pref = read_meta_data(root_directory, \"4x10mPrefWithoutStop\")\n",
    "\n",
    "sensor_fast = read_sensor_data(root_directory, \"4x10mFastWithStop\")\n",
    "meta_fast = read_meta_data(root_directory, \"4x10mFastWithStop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnosis Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add diagnosis to sensor data\n",
    "sensor_tug = add_diagnosis_to_sensor(sensor_tug, meta_tug)\n",
    "sensor_2minwalk = add_diagnosis_to_sensor(sensor_2minwalk, meta_2minwalk)\n",
    "sensor_slow = add_diagnosis_to_sensor(sensor_slow, meta_slow)\n",
    "sensor_pref = add_diagnosis_to_sensor(sensor_pref, meta_pref)\n",
    "sensor_fast = add_diagnosis_to_sensor(sensor_fast, meta_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_to_keep = [\n",
    "    \"Parkinson's disease\",\n",
    "    \"Control\",\n",
    "    \"Parkinson's disease and dementia\",\n",
    "    \"Parkinsonism unspecified\",\n",
    "    \"Secondary parkinsonism: other\"\n",
    "]\n",
    "\n",
    "label_1_diagnoses = [\n",
    "    \"Parkinson's disease\",\n",
    "    \"Parkinson's disease and dementia\",\n",
    "    \"Parkinsonism unspecified\",\n",
    "    \"Secondary parkinsonism: other\"\n",
    "]\n",
    "\n",
    "label_0_diagnoses = [\"Control\"]\n",
    "\n",
    "# Apply filtering & mapping (Diagnosis column will now be 0 or 1)\n",
    "sensor_tug = filter_and_label_diagnosis(sensor_tug, diagnosis_to_keep, label_1_diagnoses, label_0_diagnoses)\n",
    "sensor_2minwalk = filter_and_label_diagnosis(sensor_2minwalk, diagnosis_to_keep, label_1_diagnoses, label_0_diagnoses)\n",
    "sensor_slow = filter_and_label_diagnosis(sensor_slow, diagnosis_to_keep, label_1_diagnoses, label_0_diagnoses)\n",
    "sensor_pref = filter_and_label_diagnosis(sensor_pref, diagnosis_to_keep, label_1_diagnoses, label_0_diagnoses)\n",
    "sensor_fast = filter_and_label_diagnosis(sensor_fast, diagnosis_to_keep, label_1_diagnoses, label_0_diagnoses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Year of Birth (YOB) to sensor data \n",
    "sensor_tug = add_feature_to_sensor(sensor_tug, meta_tug, \"YOB\")\n",
    "sensor_2minwalk = add_feature_to_sensor(sensor_2minwalk, meta_2minwalk, \"YOB\")\n",
    "sensor_slow = add_feature_to_sensor(sensor_slow, meta_slow, \"YOB\")\n",
    "sensor_pref = add_feature_to_sensor(sensor_pref, meta_pref, \"YOB\")\n",
    "sensor_fast = add_feature_to_sensor(sensor_fast, meta_fast, \"YOB\")\n",
    "\n",
    "# Convert YOB to Age\n",
    "sensor_tug = convert_yob_to_age(sensor_tug)\n",
    "sensor_2minwalk = convert_yob_to_age(sensor_2minwalk)\n",
    "sensor_slow = convert_yob_to_age(sensor_slow)\n",
    "sensor_pref = convert_yob_to_age(sensor_pref)\n",
    "sensor_fast = convert_yob_to_age(sensor_fast)\n",
    "\n",
    "# Impute missing Age values based on unique patient averages \n",
    "sensor_tug = impute_age_by_patient_avg(sensor_tug)\n",
    "sensor_2minwalk = impute_age_by_patient_avg(sensor_2minwalk)\n",
    "sensor_slow = impute_age_by_patient_avg(sensor_slow)\n",
    "sensor_pref = impute_age_by_patient_avg(sensor_pref)\n",
    "sensor_fast = impute_age_by_patient_avg(sensor_fast)\n",
    "\n",
    "# Drop YOB\n",
    "sensor_tug.drop(columns=[\"YOB\"], inplace=True)\n",
    "sensor_2minwalk.drop(columns=[\"YOB\"], inplace=True)\n",
    "sensor_slow.drop(columns=[\"YOB\"], inplace=True)\n",
    "sensor_pref.drop(columns=[\"YOB\"], inplace=True)\n",
    "sensor_fast.drop(columns=[\"YOB\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Gender to sensor data\n",
    "sensor_tug = add_feature_to_sensor(sensor_tug, meta_tug, \"Gender\")\n",
    "sensor_2minwalk = add_feature_to_sensor(sensor_2minwalk, meta_2minwalk, \"Gender\")\n",
    "sensor_slow = add_feature_to_sensor(sensor_slow, meta_slow, \"Gender\")\n",
    "sensor_pref = add_feature_to_sensor(sensor_pref, meta_pref, \"Gender\")\n",
    "sensor_fast = add_feature_to_sensor(sensor_fast, meta_fast, \"Gender\")\n",
    "\n",
    "# Convert Gender to binary \n",
    "sensor_tug = binary_gender_encode(sensor_tug)\n",
    "sensor_2minwalk = binary_gender_encode(sensor_2minwalk)\n",
    "sensor_slow = binary_gender_encode(sensor_slow)\n",
    "sensor_pref = binary_gender_encode(sensor_pref)\n",
    "sensor_fast = binary_gender_encode(sensor_fast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add UPDRS3 to sensor data\n",
    "sensor_tug = add_feature_to_sensor(sensor_tug, meta_tug, \"MSDS-UPDRS part 3\").rename(columns={\"MSDS-UPDRS part 3\": \"UPDRS3\"})\n",
    "sensor_2minwalk = add_feature_to_sensor(sensor_2minwalk, meta_2minwalk, \"MSDS-UPDRS part 3\").rename(columns={\"MSDS-UPDRS part 3\": \"UPDRS3\"})\n",
    "sensor_slow = add_feature_to_sensor(sensor_slow, meta_slow, \"MSDS-UPDRS part 3\").rename(columns={\"MSDS-UPDRS part 3\": \"UPDRS3\"})\n",
    "sensor_pref = add_feature_to_sensor(sensor_pref, meta_pref, \"MSDS-UPDRS part 3\").rename(columns={\"MSDS-UPDRS part 3\": \"UPDRS3\"})\n",
    "sensor_fast = add_feature_to_sensor(sensor_fast, meta_fast, \"MSDS-UPDRS part 3\").rename(columns={\"MSDS-UPDRS part 3\": \"UPDRS3\"})\n",
    "\n",
    "# Impute missing UPDRS3 values \n",
    "sensor_tug = impute_updrs3_by_knn(sensor_tug)\n",
    "sensor_2minwalk = impute_updrs3_by_knn(sensor_2minwalk)\n",
    "sensor_slow = impute_updrs3_by_knn(sensor_slow)\n",
    "sensor_pref = impute_updrs3_by_knn(sensor_pref)\n",
    "sensor_fast = impute_updrs3_by_knn(sensor_fast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found existing split files for TUG. Loading data...\n",
      "Dataset Split Summary for TUG:\n",
      "  - Total Patients: 112 (Original)\n",
      "  - Train Patients: 71\n",
      "  - Validation Patients: 18\n",
      "  - Test Patients: 23\n",
      "\n",
      "Found existing split files for 2minwalk. Loading data...\n",
      "Dataset Split Summary for 2minwalk:\n",
      "  - Total Patients: 115 (Original)\n",
      "  - Train Patients: 73\n",
      "  - Validation Patients: 19\n",
      "  - Test Patients: 23\n",
      "\n",
      "Found existing split files for Slow. Loading data...\n",
      "Dataset Split Summary for Slow:\n",
      "  - Total Patients: 110 (Original)\n",
      "  - Train Patients: 70\n",
      "  - Validation Patients: 18\n",
      "  - Test Patients: 22\n",
      "\n",
      "Found existing split files for Pref. Loading data...\n",
      "Dataset Split Summary for Pref:\n",
      "  - Total Patients: 109 (Original)\n",
      "  - Train Patients: 69\n",
      "  - Validation Patients: 18\n",
      "  - Test Patients: 22\n",
      "\n",
      "Found existing split files for Fast. Loading data...\n",
      "Dataset Split Summary for Fast:\n",
      "  - Total Patients: 110 (Original)\n",
      "  - Train Patients: 70\n",
      "  - Validation Patients: 18\n",
      "  - Test Patients: 22\n"
     ]
    }
   ],
   "source": [
    "df_dict = {\n",
    "    \"TUG\": sensor_tug,\n",
    "    \"2minwalk\": sensor_2minwalk,\n",
    "    \"Slow\": sensor_slow,\n",
    "    \"Pref\": sensor_pref,\n",
    "    \"Fast\": sensor_fast,\n",
    "}\n",
    "\n",
    "save_paths = {task: f\"{root_directory}/diagnosis/df_{task}\" for task in df_dict.keys()}\n",
    "\n",
    "split_results = {}\n",
    "\n",
    "# Perform train-validation-test split \n",
    "for task_name, df in df_dict.items():\n",
    "    split_results[task_name] = split_dataset(df, dataset_name=task_name, save_path=save_paths[task_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_columns = [\n",
    "    'L_AccX_filt', 'L_AccY_filt', 'L_AccZ_filt',\n",
    "    'L_GyrX_filt', 'L_GyrY_filt', 'L_GyrZ_filt',\n",
    "    'R_AccX_filt', 'R_AccY_filt', 'R_AccZ_filt',\n",
    "    'R_GyrX_filt', 'R_GyrY_filt', 'R_GyrZ_filt'\n",
    "]\n",
    "\n",
    "# Apply normalization for Train, Validation, and Test sets\n",
    "for task_name, splits in split_results.items():\n",
    "    split_results[task_name][\"train\"] = normalize_data(splits[\"train\"], sensor_columns)\n",
    "    split_results[task_name][\"val\"] = normalize_data(splits[\"val\"], sensor_columns)\n",
    "    split_results[task_name][\"test\"] = normalize_data(splits[\"test\"], sensor_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying windowing to TUG dataset...\n",
      "TUG - Windows (Train): (477, 300, 12)\n",
      "TUG - Windows (Validation): (129, 300, 12)\n",
      "TUG - Windows (Test): (149, 300, 12)\n",
      "\n",
      "Applying windowing to 2minwalk dataset...\n",
      "2minwalk - Windows (Train): (5878, 300, 12)\n",
      "2minwalk - Windows (Validation): (1540, 300, 12)\n",
      "2minwalk - Windows (Test): (1850, 300, 12)\n",
      "\n",
      "Applying windowing to Slow dataset...\n",
      "Slow - Windows (Train): (2777, 300, 12)\n",
      "Slow - Windows (Validation): (817, 300, 12)\n",
      "Slow - Windows (Test): (918, 300, 12)\n",
      "\n",
      "Applying windowing to Pref dataset...\n",
      "Pref - Windows (Train): (1977, 300, 12)\n",
      "Pref - Windows (Validation): (578, 300, 12)\n",
      "Pref - Windows (Test): (640, 300, 12)\n",
      "\n",
      "Applying windowing to Fast dataset...\n",
      "Fast - Windows (Train): (1592, 300, 12)\n",
      "Fast - Windows (Validation): (452, 300, 12)\n",
      "Fast - Windows (Test): (511, 300, 12)\n"
     ]
    }
   ],
   "source": [
    "window_size = 300\n",
    "overlap = 150\n",
    "\n",
    "# Apply windowing for Train, Validation, and Test sets\n",
    "windowed_results = {}\n",
    "\n",
    "for task_name, splits in split_results.items():\n",
    "    print(f\"\\nApplying windowing to {task_name} dataset...\")\n",
    "\n",
    "    windows_train, labels_train, sub_ids_train = create_fixed_windows_with_overlap(\n",
    "        splits[\"train\"], sensor_columns, window_size=window_size, overlap=overlap\n",
    "    )\n",
    "    windows_val, labels_val, sub_ids_val = create_fixed_windows_with_overlap(\n",
    "        splits[\"val\"], sensor_columns, window_size=window_size, overlap=overlap\n",
    "    )\n",
    "    windows_test, labels_test, sub_ids_test = create_fixed_windows_with_overlap(\n",
    "        splits[\"test\"], sensor_columns, window_size=window_size, overlap=overlap\n",
    "    )\n",
    "\n",
    "    windowed_results[task_name] = {\n",
    "        \"windows_train\": windows_train, \"labels_train\": labels_train, \"sub_ids_train\": sub_ids_train,\n",
    "        \"windows_val\": windows_val, \"labels_val\": labels_val, \"sub_ids_val\": sub_ids_val,\n",
    "        \"windows_test\": windows_test, \"labels_test\": labels_test, \"sub_ids_test\": sub_ids_test\n",
    "    }\n",
    "    print(f\"{task_name} - Windows (Train): {windows_train.shape}\")\n",
    "    print(f\"{task_name} - Windows (Validation): {windows_val.shape}\")\n",
    "    print(f\"{task_name} - Windows (Test): {windows_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Domain (FFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying FFT to TUG dataset...\n",
      "TUG - FFT Train Shape: (477, 151, 12)\n",
      "TUG - FFT Validation Shape: (129, 151, 12)\n",
      "TUG - FFT Test Shape: (149, 151, 12)\n",
      "\n",
      "Applying FFT to 2minwalk dataset...\n",
      "2minwalk - FFT Train Shape: (5878, 151, 12)\n",
      "2minwalk - FFT Validation Shape: (1540, 151, 12)\n",
      "2minwalk - FFT Test Shape: (1850, 151, 12)\n",
      "\n",
      "Applying FFT to Slow dataset...\n",
      "Slow - FFT Train Shape: (2777, 151, 12)\n",
      "Slow - FFT Validation Shape: (817, 151, 12)\n",
      "Slow - FFT Test Shape: (918, 151, 12)\n",
      "\n",
      "Applying FFT to Pref dataset...\n",
      "Pref - FFT Train Shape: (1977, 151, 12)\n",
      "Pref - FFT Validation Shape: (578, 151, 12)\n",
      "Pref - FFT Test Shape: (640, 151, 12)\n",
      "\n",
      "Applying FFT to Fast dataset...\n",
      "Fast - FFT Train Shape: (1592, 151, 12)\n",
      "Fast - FFT Validation Shape: (452, 151, 12)\n",
      "Fast - FFT Test Shape: (511, 151, 12)\n"
     ]
    }
   ],
   "source": [
    "# Apply FFT \n",
    "fft_results = {}\n",
    "\n",
    "for task_name, windows in windowed_results.items():\n",
    "    print(f\"\\nApplying FFT to {task_name} dataset...\")\n",
    "\n",
    "    fft_train, freq_bins = apply_fft_to_windows(windows[\"windows_train\"])  # Full FFT spectrum\n",
    "    fft_val, _ = apply_fft_to_windows(windows[\"windows_val\"])\n",
    "    fft_test, _ = apply_fft_to_windows(windows[\"windows_test\"])\n",
    "\n",
    "    fft_results[task_name] = {\n",
    "        \"fft_train\": fft_train, \"labels_train\": windows[\"labels_train\"], \"sub_ids_train\": windows[\"sub_ids_train\"],\n",
    "        \"fft_val\": fft_val, \"labels_val\": windows[\"labels_val\"], \"sub_ids_val\": windows[\"sub_ids_val\"],\n",
    "        \"fft_test\": fft_test, \"labels_test\": windows[\"labels_test\"], \"sub_ids_test\": windows[\"sub_ids_test\"],\n",
    "        \"freq_bins\": freq_bins  \n",
    "    }\n",
    "\n",
    "    print(f\"{task_name} - FFT Train Shape: {fft_train.shape}\")\n",
    "    print(f\"{task_name} - FFT Validation Shape: {fft_val.shape}\")\n",
    "    print(f\"{task_name} - FFT Test Shape: {fft_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting time-domain features for TUG dataset...\n",
      "TUG - Time-Domain Features Shape (Train): (477, 156)\n",
      "\n",
      "Extracting time-domain features for 2minwalk dataset...\n",
      "2minwalk - Time-Domain Features Shape (Train): (5878, 156)\n",
      "\n",
      "Extracting time-domain features for Slow dataset...\n",
      "Slow - Time-Domain Features Shape (Train): (2777, 156)\n",
      "\n",
      "Extracting time-domain features for Pref dataset...\n",
      "Pref - Time-Domain Features Shape (Train): (1977, 156)\n",
      "\n",
      "Extracting time-domain features for Fast dataset...\n",
      "Fast - Time-Domain Features Shape (Train): (1592, 156)\n"
     ]
    }
   ],
   "source": [
    "# Apply time-domain feature extraction \n",
    "time_features_results = {}\n",
    "\n",
    "for task_name, windows in windowed_results.items():\n",
    "    print(f\"\\nExtracting time-domain features for {task_name} dataset...\")\n",
    "\n",
    "    X_train = extract_time_domain_features(windows[\"windows_train\"])\n",
    "    X_val = extract_time_domain_features(windows[\"windows_val\"])\n",
    "    X_test = extract_time_domain_features(windows[\"windows_test\"])\n",
    "\n",
    "    time_features_results[task_name] = {\n",
    "        \"X_train\": X_train, \"labels_train\": windows[\"labels_train\"], \"sub_ids_train\": windows[\"sub_ids_train\"],\n",
    "        \"X_val\": X_val, \"labels_val\": windows[\"labels_val\"], \"sub_ids_val\": windows[\"sub_ids_val\"],\n",
    "        \"X_test\": X_test, \"labels_test\": windows[\"labels_test\"], \"sub_ids_test\": windows[\"sub_ids_test\"]\n",
    "    }\n",
    "\n",
    "    print(f\"{task_name} - Time-Domain Features Shape (Train): {X_train.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting FFT features for TUG dataset...\n",
      "TUG - FFT Feature Shape (Train): (477, 60)\n",
      "\n",
      "Extracting FFT features for 2minwalk dataset...\n",
      "2minwalk - FFT Feature Shape (Train): (5878, 60)\n",
      "\n",
      "Extracting FFT features for Slow dataset...\n",
      "Slow - FFT Feature Shape (Train): (2777, 60)\n",
      "\n",
      "Extracting FFT features for Pref dataset...\n",
      "Pref - FFT Feature Shape (Train): (1977, 60)\n",
      "\n",
      "Extracting FFT features for Fast dataset...\n",
      "Fast - FFT Feature Shape (Train): (1592, 60)\n"
     ]
    }
   ],
   "source": [
    "# Apply FFT feature extraction \n",
    "fft_features_results = {}\n",
    "\n",
    "for task_name, fft_data in fft_results.items():\n",
    "    print(f\"\\nExtracting FFT features for {task_name} dataset...\")\n",
    "\n",
    "    X_train = extract_fft_features(fft_data[\"fft_train\"], fft_data[\"freq_bins\"])\n",
    "    X_val = extract_fft_features(fft_data[\"fft_val\"], fft_data[\"freq_bins\"])\n",
    "    X_test = extract_fft_features(fft_data[\"fft_test\"], fft_data[\"freq_bins\"])\n",
    "\n",
    "    fft_features_results[task_name] = {\n",
    "        \"X_train\": X_train, \"labels_train\": fft_data[\"labels_train\"], \"sub_ids_train\": fft_data[\"sub_ids_train\"],\n",
    "        \"X_val\": X_val, \"labels_val\": fft_data[\"labels_val\"], \"sub_ids_val\": fft_data[\"sub_ids_val\"],\n",
    "        \"X_test\": X_test, \"labels_test\": fft_data[\"labels_test\"], \"sub_ids_test\": fft_data[\"sub_ids_test\"]\n",
    "    }\n",
    "\n",
    "    print(f\"{task_name} - FFT Feature Shape (Train): {X_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
